{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffcc35fb",
   "metadata": {},
   "source": [
    "# Job Interview Guide — Workshop\n",
    "\n",
    "*Generated on 2025-10-17 17:47:07*\n",
    "\n",
    "This workshop has two parts:\n",
    "1. A transcript with the prompts and the full quiz session.\n",
    "2. Hands-on exercises for regression metrics and model evaluation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420898fc",
   "metadata": {},
   "source": [
    "## Transcript\n",
    "\n",
    "Below is the content you requested. It captures the prompts and the full quiz session.\n",
    "\n",
    "### Initial task prompt from user\n",
    "\n",
    "You are a **senior Data Scientist and Machine Learning Engineer** who also acts as a **technical interviewer**.  \n",
    "I am a **junior Data Scientist and Machine Learning Engineer**, recently graduated and preparing for an **ML Specialist interview**.\n",
    "\n",
    "You will guide me through an interactive study and evaluation process.\n",
    "\n",
    "Two files are attached:\n",
    "\n",
    "* **StudyGuide.txt** – contains the study guides.\n",
    "* **StudyMaterials.zip** – contains the HTML version Jupyter Notebook workshops.\n",
    "\n",
    "Your tasks:\n",
    "\n",
    "1. **Analyze the materials**\n",
    "\n",
    "   * Unzip and review the Jupyter Notebooks.\n",
    "   * Create a **500-word summary** explaining the main learning content and technical focus.\n",
    "\n",
    "2. **Cross-reference with the study guides**\n",
    "\n",
    "   * Unzip and review the Study Guide.\n",
    "   * Match each study guide topic with related content in the Jupyter Notebooks.\n",
    "   * Produce a **comparison table** showing whether each topic is covered or missing.\n",
    "\n",
    "3. **Create a quiz**\n",
    "\n",
    "   * Write **15 multiple-choice questions** (A–E) based on the combined knowledge from both sources.\n",
    "   * Each question should test understanding relevant to a real-world ML interview.\n",
    "\n",
    "4. **Interactive quiz mode**\n",
    "\n",
    "   * Do **not** produce the quiz immediately.\n",
    "   * Instead, start a quiz session and ask **one question at a time**.\n",
    "   * I will answer each question.\n",
    "   * Track my score.\n",
    "   * After the quiz ends, show my **total score** and generate a **Jupyter Notebook with exercises** targeting my weak areas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88266ef2",
   "metadata": {},
   "source": [
    "### User: Start quiz\n",
    "\n",
    "**User:** Yes, start the quiz session now\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2feabae6",
   "metadata": {},
   "source": [
    "\n",
    "### Quiz session\n",
    "\n",
    "**Q1.** Purpose of the validation dataset?  \n",
    "A. Train the model by updating parameters  \n",
    "B. Evaluate the model on unseen data for the first time  \n",
    "C. Tune hyperparameters and choose the best model configuration  \n",
    "D. Measure model drift after deployment  \n",
    "E. Store outliers removed from training data  \n",
    "\n",
    "**User answer:** B  \n",
    "**Correct:** C  \n",
    "**Explanation:** Validation is for model selection and hyperparameter tuning. The test set is for first unbiased evaluation.\n",
    "\n",
    "---\n",
    "\n",
    "**Q2.** Best description of a non-parametric model?  \n",
    "A. Fixed number of parameters independent of data size  \n",
    "B. Assumes predefined functional form  \n",
    "C. Learns patterns directly from data without a fixed structure  \n",
    "D. Always faster to train  \n",
    "E. Always interpretable and less prone to overfitting  \n",
    "\n",
    "**User answer:** C  \n",
    "**Correct:** C  \n",
    "**Explanation:** Non-parametric models adapt to data without a fixed functional form.\n",
    "\n",
    "---\n",
    "\n",
    "**Q3.** Role of learning rate in Gradient Descent?  \n",
    "A. Determines epochs  \n",
    "B. Controls step size for parameter updates  \n",
    "C. Defines number of parameters  \n",
    "D. Measures curvature of loss  \n",
    "E. Acts as regularizer  \n",
    "\n",
    "**User answer:** B  \n",
    "**Correct:** B  \n",
    "**Explanation:** Step size per update.\n",
    "\n",
    "---\n",
    "\n",
    "**Q4.** Appropriate loss for classification?  \n",
    "A. MSE  \n",
    "B. MAE  \n",
    "C. Cross-Entropy  \n",
    "D. Huber  \n",
    "E. MAPE  \n",
    "\n",
    "**User answer:** C  \n",
    "**Correct:** C  \n",
    "**Explanation:** Cross-Entropy penalizes confident wrong predictions.\n",
    "\n",
    "---\n",
    "\n",
    "**Q5.** High R² means?  \n",
    "A. Low variance and overfitting  \n",
    "B. Model explains most target variance  \n",
    "C. Poor generalization  \n",
    "D. Residuals are uncorrelated  \n",
    "E. Loss reached a local minimum  \n",
    "\n",
    "**User answer:** A  \n",
    "**Correct:** B  \n",
    "**Explanation:** R² measures variance explained. It does not by itself indicate overfitting.\n",
    "\n",
    "---\n",
    "\n",
    "**Q6.** OLS is:  \n",
    "A. Iterative gradient update algorithm  \n",
    "B. Closed-form method minimizing squared errors  \n",
    "C. Probabilistic model for binary classification  \n",
    "D. Minimizes absolute deviations  \n",
    "E. Cross-validation strategy  \n",
    "\n",
    "**User answer:** B  \n",
    "**Correct:** B\n",
    "\n",
    "---\n",
    "\n",
    "**Q7.** Key advantage of parametric models:  \n",
    "A. Always outperform non-parametric models  \n",
    "B. No assumptions about data  \n",
    "C. Simple, interpretable, and efficient  \n",
    "D. Automatically adapt to nonlinear data  \n",
    "E. Avoid underfitting regardless of complexity  \n",
    "\n",
    "**User answer:** C  \n",
    "**Correct:** C\n",
    "\n",
    "---\n",
    "\n",
    "**Q8.** Purpose of k-fold CV repeats:  \n",
    "A. Reduce bias by growing training set  \n",
    "B. Ensure all data points are used for training and validation  \n",
    "C. Find optimal learning rate automatically  \n",
    "D. Prevent fast convergence  \n",
    "E. Balance classes during training  \n",
    "\n",
    "**User answer:** B  \n",
    "**Correct:** B\n",
    "\n",
    "---\n",
    "\n",
    "**Q9.** Main goal of optimization:  \n",
    "A. Select features automatically  \n",
    "B. Find parameters that minimize loss  \n",
    "C. Normalize inputs  \n",
    "D. Compute confidence intervals  \n",
    "E. Improve interpretability via regularization  \n",
    "\n",
    "**User answer:** B  \n",
    "**Correct:** B\n",
    "\n",
    "---\n",
    "\n",
    "**Q10.** Most robust to outliers in regression:  \n",
    "A. MSE  \n",
    "B. MAE  \n",
    "C. RMSE  \n",
    "D. R²  \n",
    "E. MAPE  \n",
    "\n",
    "**User answer:** A  \n",
    "**Correct:** B  \n",
    "**Explanation:** MAE does not square errors, so outliers have less impact.\n",
    "\n",
    "---\n",
    "\n",
    "**Q11.** Primary limitation of Gradient Descent vs OLS:  \n",
    "A. Cannot handle large datasets  \n",
    "B. Requires iterative updates and may converge slowly  \n",
    "C. Cannot apply to nonlinear models  \n",
    "D. Does not compute gradients  \n",
    "E. Non-deterministic results  \n",
    "\n",
    "**User answer:** B  \n",
    "**Correct:** B\n",
    "\n",
    "---\n",
    "\n",
    "**Q12.** Role of sigmoid in Logistic Regression:  \n",
    "A. Create polynomial features  \n",
    "B. Normalize features  \n",
    "C. Map real numbers to probabilities in [0,1]  \n",
    "D. Measure loss  \n",
    "E. Balance classes  \n",
    "\n",
    "**User answer:** C  \n",
    "**Correct:** C\n",
    "\n",
    "---\n",
    "\n",
    "**Q13.** Early stopping means:  \n",
    "A. Stop at 100% test accuracy  \n",
    "B. Stop when validation performance stops improving  \n",
    "C. Stop after fixed epochs  \n",
    "D. Stop when training loss is lowest  \n",
    "E. Stop when gradient is zero  \n",
    "\n",
    "**User answer:** B  \n",
    "**Correct:** B\n",
    "\n",
    "---\n",
    "\n",
    "**Q14.** True about R²:  \n",
    "A. Measures average absolute error  \n",
    "B. Increases as irrelevant features are added  \n",
    "C. Compares model vs predicting the mean  \n",
    "D. Only for classification  \n",
    "E. Can never be negative  \n",
    "\n",
    "**User answer:** C  \n",
    "**Correct:** C\n",
    "\n",
    "---\n",
    "\n",
    "**Q15.** Self-supervised learning is:  \n",
    "A. Learning with no feedback  \n",
    "B. Learning with labeled data  \n",
    "C. Predicting masked or missing parts of input as supervision  \n",
    "D. Rewards or penalties for actions  \n",
    "E. Reinforcement from human evaluators  \n",
    "\n",
    "**User answer:** C  \n",
    "**Correct:** C\n",
    "\n",
    "**Final score:** 12 / 15\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eaf6434",
   "metadata": {},
   "source": [
    "### Follow-up prompts\n",
    "\n",
    "- **User:** Yes, proceed  \n",
    "- **Assistant:** Generated targeted exercises notebook  \n",
    "- **User:** Generate the notebook again. Call it \"JobInterviewGuide_Workshop.ipynb\". Include markdown with the contents of the entire quiz session and all the prompts I issued herein.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0220ea1c",
   "metadata": {},
   "source": [
    "## Workshop Exercises: Regression Metrics and Model Evaluation\n",
    "\n",
    "You will practice:\n",
    "- MAE vs MSE robustness to outliers.\n",
    "- R² by formula and with a mean baseline.\n",
    "- Proper train, validation, and test flow.\n",
    "- Validation curve for polynomial degree.\n",
    "- 10-fold cross-validation stability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f697b2",
   "metadata": {},
   "source": [
    "### Environment setup\n",
    "\n",
    "Install if needed:\n",
    "```\n",
    "pip install numpy pandas scikit-learn matplotlib\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b670df",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f737d4a5",
   "metadata": {},
   "source": [
    "### Data generation with injected outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54cc9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Base linear relationship\n",
    "n = 300\n",
    "X = np.linspace(-4, 4, n).reshape(-1, 1)\n",
    "y_true = 3.0 * X.squeeze() + 5.0\n",
    "noise = np.random.normal(0, 1.0, size=n)\n",
    "y = y_true + noise\n",
    "\n",
    "# Inject outliers\n",
    "outlier_idx = np.random.choice(np.arange(n), size=6, replace=False)\n",
    "y[outlier_idx] += np.random.choice([15, -15, 20, -20], size=6)\n",
    "\n",
    "# DataFrame\n",
    "df = pd.DataFrame({\"x\": X.squeeze(), \"y\": y, \"is_outlier\": 0})\n",
    "df.loc[outlier_idx, \"is_outlier\"] = 1\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38aff48d",
   "metadata": {},
   "source": [
    "### Plot: data with outliers highlighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b05f0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure()\n",
    "plt.scatter(df[\"x\"], df[\"y\"], s=12, label=\"data\")\n",
    "plt.scatter(df.loc[df.is_outlier==1, \"x\"], df.loc[df.is_outlier==1, \"y\"], s=40, marker=\"x\", label=\"outliers\")\n",
    "plt.title(\"Synthetic data with injected outliers\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0868f1bd",
   "metadata": {},
   "source": [
    "### Exercise 1. MAE vs MSE robustness to outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9f40a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Fit model on all data\n",
    "X_all, y_all = df[[\"x\"]].values, df[\"y\"].values\n",
    "model = LinearRegression()\n",
    "model.fit(X_all, y_all)\n",
    "y_pred_all = model.predict(X_all)\n",
    "\n",
    "mae_all = mean_absolute_error(y_all, y_pred_all)\n",
    "mse_all = mean_squared_error(y_all, y_pred_all)\n",
    "rmse_all = mean_squared_error(y_all, y_pred_all, squared=False)\n",
    "\n",
    "# Remove outliers and recompute\n",
    "mask = df[\"is_outlier\"] == 0\n",
    "X_clean, y_clean = df.loc[mask, [\"x\"]].values, df.loc[mask, \"y\"].values\n",
    "model_clean = LinearRegression().fit(X_clean, y_clean)\n",
    "y_pred_clean = model_clean.predict(X_clean)\n",
    "\n",
    "mae_clean = mean_absolute_error(y_clean, y_pred_clean)\n",
    "mse_clean = mean_squared_error(y_clean, y_pred_clean)\n",
    "rmse_clean = mean_squared_error(y_clean, y_pred_clean, squared=False)\n",
    "\n",
    "summary = pd.DataFrame({\n",
    "    \"metric\": [\"MAE\", \"MSE\", \"RMSE\"],\n",
    "    \"with_outliers\": [mae_all, mse_all, rmse_all],\n",
    "    \"without_outliers\": [mae_clean, mse_clean, rmse_clean]\n",
    "})\n",
    "summary[\"pct_change_when_removing_outliers\"] = 100 * (summary[\"with_outliers\"] - summary[\"without_outliers\"]) / summary[\"with_outliers\"]\n",
    "summary.round(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba459c65",
   "metadata": {},
   "source": [
    "### Plot residuals: with and without outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352383e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "res_all = y_all - y_pred_all\n",
    "res_clean = y_clean - y_pred_clean\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(res_all, bins=30)\n",
    "plt.title(\"Residuals with outliers\")\n",
    "plt.xlabel(\"residual\")\n",
    "plt.ylabel(\"count\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(res_clean, bins=30)\n",
    "plt.title(\"Residuals without outliers\")\n",
    "plt.xlabel(\"residual\")\n",
    "plt.ylabel(\"count\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6948b1",
   "metadata": {},
   "source": [
    "**Guiding questions**\n",
    "- Which metric changed the most after removing outliers, MAE or MSE or RMSE?\n",
    "- Which metric is more robust to outliers, and why?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e13cd6",
   "metadata": {},
   "source": [
    "### Exercise 2. R² interpretation and manual computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e6196d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[[\"x\"]].values, df[\"y\"].values, test_size=0.25, random_state=7)\n",
    "\n",
    "# Fit\n",
    "lin = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "# Library R²\n",
    "yhat_test = lin.predict(X_test)\n",
    "r2_lib = r2_score(y_test, yhat_test)\n",
    "\n",
    "# Manual R²\n",
    "ybar = np.mean(y_test)\n",
    "ss_res = np.sum((y_test - yhat_test)**2)\n",
    "ss_tot = np.sum((y_test - ybar)**2)\n",
    "r2_manual = 1 - ss_res / ss_tot\n",
    "\n",
    "# Baseline: mean predictor\n",
    "yhat_base = np.full_like(y_test, fill_value=np.mean(y_train))\n",
    "r2_base = r2_score(y_test, yhat_base)\n",
    "\n",
    "pd.DataFrame({\n",
    "    \"metric\": [\"R2_library\", \"R2_manual\", \"R2_baseline_mean\"],\n",
    "    \"value\": [r2_lib, r2_manual, r2_base]\n",
    "}).round(4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474de9a6",
   "metadata": {},
   "source": [
    "### Plot: predictions vs ground truth on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fb6765",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure()\n",
    "plt.scatter(X_test.squeeze(), y_test, s=12, label=\"y_test\")\n",
    "plt.scatter(X_test.squeeze(), yhat_test, s=12, label=\"y_pred\")\n",
    "plt.title(\"Test set: true vs predicted\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceed73e4",
   "metadata": {},
   "source": [
    "**Guiding questions**\n",
    "- Does the manual R² match the library value up to floating-point tolerance?\n",
    "- What does the baseline R² tell you about the usefulness of the learned model?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa72ca7a",
   "metadata": {},
   "source": [
    "### Exercise 3. Proper Train/Validation/Test flow with polynomial degree selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325d95d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split train/val/test\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(df[[\"x\"]].values, df[\"y\"].values, test_size=0.35, random_state=1)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=1)\n",
    "\n",
    "val_rows = []\n",
    "best = {\"deg\": None, \"mae\": np.inf, \"r2\": -np.inf}\n",
    "\n",
    "for deg in [1,2,3,4,5]:\n",
    "    pipe = make_pipeline(PolynomialFeatures(deg, include_bias=False), LinearRegression())\n",
    "    pipe.fit(X_train, y_train)\n",
    "    yv = pipe.predict(X_val)\n",
    "    mae = mean_absolute_error(y_val, yv)\n",
    "    r2v = r2_score(y_val, yv)\n",
    "    val_rows.append({\"degree\": deg, \"val_MAE\": mae, \"val_R2\": r2v})\n",
    "    if mae < best[\"mae\"]:\n",
    "        best = {\"deg\": deg, \"mae\": mae, \"r2\": r2v}\n",
    "\n",
    "val_table = pd.DataFrame(val_rows).sort_values(\"val_MAE\").reset_index(drop=True)\n",
    "\n",
    "# Refit best on train+val and evaluate on test once\n",
    "X_trval = np.vstack([X_train, X_val])\n",
    "y_trval = np.concatenate([y_train, y_val])\n",
    "best_pipe = make_pipeline(PolynomialFeatures(best[\"deg\"], include_bias=False), LinearRegression())\n",
    "best_pipe.fit(X_trval, y_trval)\n",
    "yt = best_pipe.predict(X_test)\n",
    "\n",
    "final_mae = mean_absolute_error(y_test, yt)\n",
    "final_mse = mean_squared_error(y_test, yt)\n",
    "final_rmse = mean_squared_error(y_test, yt, squared=False)\n",
    "final_r2 = r2_score(y_test, yt)\n",
    "\n",
    "val_table.round(4), {\"best_degree\": best[\"deg\"], \"test_MAE\": final_mae, \"test_MSE\": final_mse, \"test_RMSE\": final_rmse, \"test_R2\": final_r2}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9696fdb5",
   "metadata": {},
   "source": [
    "### Plot: validation curve by polynomial degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e366ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "degrees = val_table[\"degree\"].values\n",
    "maes = val_table[\"val_MAE\"].values\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(degrees, maes, marker=\"o\")\n",
    "plt.title(\"Validation MAE vs Polynomial degree\")\n",
    "plt.xlabel(\"degree\")\n",
    "plt.ylabel(\"validation MAE\")\n",
    "plt.xticks(degrees)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c80f72",
   "metadata": {},
   "source": [
    "**Guiding questions**\n",
    "- Does a higher degree always improve validation MAE?\n",
    "- Why should you refit on train+val before a single final test?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f5543e",
   "metadata": {},
   "source": [
    "### Exercise 4. 10-fold cross-validation stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5721c08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "best_deg = int(val_table.sort_values(\"val_MAE\").iloc[0][\"degree\"])\n",
    "pipe_cv = make_pipeline(PolynomialFeatures(best_deg, include_bias=False), LinearRegression())\n",
    "\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=123)\n",
    "neg_mae_scores = cross_val_score(pipe_cv, df[[\"x\"]].values, df[\"y\"].values, cv=cv, scoring=\"neg_mean_absolute_error\")\n",
    "mae_scores = -neg_mae_scores\n",
    "\n",
    "pd.DataFrame({\n",
    "    \"fold\": np.arange(1, len(mae_scores)+1),\n",
    "    \"MAE\": mae_scores\n",
    "}).assign(mean=mae_scores.mean(), std=mae_scores.std())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48fbfe80",
   "metadata": {},
   "source": [
    "### Optional Exercise 5. Overfitting demo\n",
    "\n",
    "- Try a high degree, such as 12 or 15, and repeat the process.\n",
    "- Compare train MAE vs validation MAE to see overfitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506f29ed",
   "metadata": {},
   "source": [
    "## What to submit\n",
    "\n",
    "- The executed notebook with answers to the guiding questions.\n",
    "- Plots for the residuals, prediction vs truth, validation curve, and CV stability.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
